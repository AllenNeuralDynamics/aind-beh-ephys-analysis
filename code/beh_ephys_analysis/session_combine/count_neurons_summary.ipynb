{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/root/capsule/code/beh_ephys_analysis')\n",
    "from harp.clock import decode_harp_clock, align_timestamps_to_anchor_points\n",
    "from open_ephys.analysis import Session\n",
    "import datetime\n",
    "from aind_ephys_rig_qc.temporal_alignment import search_harp_line\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pandas as pd\n",
    "from pynwb import NWBFile, TimeSeries, NWBHDF5IO\n",
    "from scipy.io import loadmat\n",
    "from scipy.stats import zscore\n",
    "import ast\n",
    "from utils.plot_utils import combine_pdf_big\n",
    "\n",
    "from open_ephys.analysis import Session\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "import json\n",
    "import seaborn as sns\n",
    "from PyPDF2 import PdfMerger\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "import re\n",
    "from aind_dynamic_foraging_basic_analysis.plot.plot_foraging_session import plot_foraging_session\n",
    "from aind_dynamic_foraging_data_utils.nwb_utils import load_nwb_from_filename\n",
    "from hdmf_zarr.nwb import NWBZarrIO\n",
    "from utils.beh_functions import session_dirs, parseSessionID, load_model_dv, makeSessionDF, get_session_tbl, get_unit_tbl, get_history_from_nwb\n",
    "from utils.ephys_functions import*\n",
    "from utils.opto_utils import opto_metrics\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.stats as stats\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import time\n",
    "import spikeinterface as si\n",
    "import shutil \n",
    "import seaborn as sns\n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import zscore\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make combined session-unit table\n",
    "df = pd.read_csv('/root/capsule/code/data_management/session_assets.csv')\n",
    "# df = df[-10:]\n",
    "session_ids = df['session_id'].values\n",
    "behs = df['behavior'].values\n",
    "exclude = ['ecephys_717120_2024-03-06_12-23-53', 'ecephys_713854_2024-03-08_14-54-25', 'ecephys_713854_2024-03-08_16-20-33', 'behavior_754897_2025-03-15_11-32-18', 'behavior_752014_2025-03-28_11-04-59']\n",
    "session_ids, behs = zip(*[\n",
    "    (session, beh)\n",
    "    for session, beh in zip(session_ids, behs)\n",
    "    if isinstance(session, str) and session not in exclude\n",
    "])\n",
    "session_ids = list(session_ids)\n",
    "behs = list(behs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ecephys_713854_2024-03-05_12-01-40\n",
      "Processing ecephys_713854_2024-03-05_13-01-09\n",
      "Processing ecephys_713854_2024-03-05_13-31-20\n",
      "Processing ecephys_717120_2024-03-06_12-54-27\n",
      "Processing ecephys_717120_2024-03-07_12-12-02\n",
      "Processing ecephys_713854_2024-03-08_15-43-01\n",
      "There is no nwb file in the curated directory.\n",
      "Processing ecephys_684930_2023-09-27_10-04-04\n",
      "Processing ecephys_684930_2023-09-28_11-45-27\n",
      "Processing ecephys_684930_2023-09-28_12-44-15\n",
      "Processing ecephys_687697_2023-09-15_11-30-06\n",
      "Processing ecephys_687697_2023-09-15_12-36-06\n",
      "Processing ecephys_691893_2023-10-05_12-46-57\n",
      "Processing ecephys_691893_2023-10-06_13-48-18\n",
      "Processing behavior_716325_2024-05-31_10-31-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/hdmf/spec/namespace.py:583: UserWarning: Ignoring the following cached namespace(s) because another version is already loaded:\n",
      "core - cached version: 2.6.0-alpha, loaded version: 2.7.0\n",
      "The loaded extension(s) may not be compatible with the cached extension(s) in the file. Please check the extension documentation and ignore this warning if these versions are compatible.\n",
      "  self.warn_for_ignored_namespaces(ignored_namespaces)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing behavior_717121_2024-06-15_10-00-58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/hdmf/spec/namespace.py:583: UserWarning: Ignoring the following cached namespace(s) because another version is already loaded:\n",
      "core - cached version: 2.6.0-alpha, loaded version: 2.7.0\n",
      "The loaded extension(s) may not be compatible with the cached extension(s) in the file. Please check the extension documentation and ignore this warning if these versions are compatible.\n",
      "  self.warn_for_ignored_namespaces(ignored_namespaces)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are multiple recordings in the raw nwb directory. Please specify the recording you would like to use.\n",
      "Processing behavior_751004_2024-12-19_11-50-37\n",
      "Processing behavior_751004_2024-12-20_13-26-11\n",
      "Processing behavior_751004_2024-12-21_13-28-28\n",
      "Processing behavior_751004_2024-12-22_13-09-17\n",
      "Processing behavior_751004_2024-12-23_14-20-03\n",
      "Processing behavior_751769_2025-01-16_11-32-05\n",
      "Processing behavior_751769_2025-01-17_11-37-39\n",
      "Processing behavior_751769_2025-01-18_10-15-25\n",
      "Processing behavior_758017_2025-02-04_11-57-38\n",
      "Processing behavior_758017_2025-02-05_11-42-34\n",
      "Processing behavior_758017_2025-02-06_11-26-14\n",
      "Processing behavior_758017_2025-02-07_14-11-08\n",
      "Processing behavior_751766_2025-02-11_11-53-38\n",
      "Processing behavior_751766_2025-02-13_11-31-21\n",
      "Processing behavior_751766_2025-02-14_11-37-11\n",
      "There is no nwb file in the raw directory.\n",
      "Processing behavior_751181_2025-02-25_12-12-35\n",
      "Processing behavior_751181_2025-02-26_11-51-19\n",
      "Processing behavior_751181_2025-02-27_11-24-47\n",
      "Processing behavior_754897_2025-03-11_12-07-41\n",
      "Processing behavior_754897_2025-03-12_12-23-15\n",
      "Processing behavior_754897_2025-03-13_11-20-42\n",
      "Processing behavior_754897_2025-03-14_11-28-53\n",
      "Processing behavior_758018_2025-03-19_11-16-44\n",
      "Processing behavior_758018_2025-03-20_11-53-05\n",
      "Processing behavior_758018_2025-03-21_11-00-34\n",
      "Processing behavior_752014_2025-03-25_12-09-20\n",
      "Processing behavior_752014_2025-03-26_11-18-57\n",
      "Processing behavior_752014_2025-03-27_12-03-59\n",
      "Processing behavior_761038_2025-04-15_10-25-11\n",
      "Processing behavior_761038_2025-04-16_10-39-10\n",
      "Processing behavior_761038_2025-04-17_11-03-16\n",
      "Processing behavior_761038_2025-04-18_12-37-39\n",
      "Processing ecephys_763360_2025-04-15_12-16-29\n",
      "Processing ecephys_763360_2025-04-16_13-29-55\n",
      "Processing behavior_782394_2025-04-22_10-53-28\n",
      "Processing behavior_782394_2025-04-23_10-51-17\n",
      "Processing behavior_782394_2025-04-24_12-07-34\n",
      "Processing behavior_782394_2025-04-25_11-13-21\n",
      "Processing behavior_763590_2025-05-01_10-59-18\n",
      "Processing behavior_763590_2025-05-02_11-07-09\n",
      "Processing behavior_781166_2025-05-13_14-04-27\n",
      "Processing behavior_781166_2025-05-14_14-18-28\n",
      "Processing behavior_781166_2025-05-15_14-20-51\n",
      "Processing behavior_781166_2025-05-16_14-16-34\n",
      "Processing behavior_785956_2025-05-20_14-15-19\n",
      "Processing behavior_785956_2025-05-21_13-42-02\n",
      "Processing behavior_785956_2025-05-23_13-45-21\n"
     ]
    }
   ],
   "source": [
    "# loop through all sessions\n",
    "# count number of units pass qc and opto qc\n",
    "# check if exist in longer than 100 sessions \n",
    "# check if exist in pre/post stimulation\n",
    "# check if exist in anti-dromic stimulation\n",
    "all_sessions = []\n",
    "all_units = []\n",
    "all_qc_pass_units = [] # default qc pass\n",
    "all_real_units = [] # not noise, not artifact\n",
    "all_tagged_units = []\n",
    "in_behavior = []\n",
    "trial_count = []\n",
    "opto_tagging_pre = []\n",
    "opto_tagging_post = []\n",
    "anti = []\n",
    "probes = []\n",
    "all_p_max = []\n",
    "all_p_mean = []\n",
    "all_lat_max_p = []  # latency of max p response\n",
    "isi_v = []\n",
    "all_eu = []\n",
    "all_corr = []\n",
    "all_amp = []\n",
    "all_peak = []\n",
    "all_wf = []\n",
    "all_wf_aligned = []\n",
    "all_wf_raw = []\n",
    "all_wf_2d = []\n",
    "all_wf_2d_raw = []\n",
    "all_peak_raw = []\n",
    "all_amp_raw = []\n",
    "y_loc = []\n",
    "in_df = []\n",
    "rec_side = []\n",
    "top = []\n",
    "bottom = []\n",
    "snr = []\n",
    "all_tag_loc = []\n",
    "all_fr = []\n",
    "all_decoder = []\n",
    "all_peak_channel_ind = []\n",
    "resp_p_all_conditions = []\n",
    "mean_p_all_conditions = []\n",
    "resp_lat_all_conditions = []\n",
    "corr_all_conditions = []\n",
    "eu_all_conditions = []\n",
    "# p_resp_thresh = 0.5\n",
    "# lat_resp_thresh = 0.02\n",
    "\n",
    "target = 'soma'\n",
    "for session, beh in zip(session_ids, behs):\n",
    "    session_dir = session_dirs(session)\n",
    "    if session_dir['curated_dir_curated'] is not None:\n",
    "        # if 'behavior_761038_2025-04-15_10-25-11' in session:\n",
    "        #     print('behavior_761038_2025-04-15_10-25-11')\n",
    "        print(f'Processing {session}')\n",
    "        data_type = 'curated'\n",
    "        # if session_dir['curated_dir_curated'] is not None:\n",
    "        #     data_type = 'curated'\n",
    "        # elif session_dir['curated_dir_raw'] is not None:\n",
    "        #     data_type = 'raw'\n",
    "        # else:\n",
    "        #     continue\n",
    "\n",
    "        unit_tbl = get_unit_tbl(session, data_type)\n",
    "        opto_metrics_session = opto_metrics(session, data_type=data_type)\n",
    "        unit_nwb = load_nwb_from_filename(session_dir['nwb_dir_curated'])\n",
    "        unit_temps = unit_nwb.units[:]['waveform_mean'].values\n",
    "        peak_C = [np.argmax(np.ptp(curr_wf, axis=0)) if isinstance(curr_wf, np.ndarray) else None for curr_wf in unit_temps]  # peak channel index\n",
    "        # print(np.shape(unit_temps[0]))\n",
    "        all_units.extend(unit_tbl['unit_id'].tolist())\n",
    "        all_qc_pass_units.extend(unit_tbl['default_qc'].tolist())  # default qc pass\n",
    "        # all_real_units.extend(unit_tbl['real_unit'].tolist())\n",
    "        all_tagged_units.extend(unit_tbl['tagged_loc'].tolist())  # tagged location (e.g. 'soma', 'axon', 'unspecified')\n",
    "        all_sessions.extend([session]*len(unit_tbl))\n",
    "        all_peak_channel_ind.extend(peak_C)  # peak channel index\n",
    "        if 'p_max' not in unit_tbl.columns:\n",
    "            all_p_max.extend(unit_tbl['p_max_x'].tolist())\n",
    "            all_p_mean.extend(unit_tbl['p_mean_x'].tolist())\n",
    "            all_lat_max_p.extend(unit_tbl['lat_max_p_x'].tolist())\n",
    "            all_eu.extend(unit_tbl['euc_max_p_x'].tolist())\n",
    "            all_corr.extend(unit_tbl['corr_max_p_x'].tolist())\n",
    "            peaks = unit_tbl['peak_x'].values\n",
    "            amp = unit_tbl['amp_x'].values\n",
    "        else: \n",
    "            all_p_max.extend(unit_tbl['p_max'].tolist())\n",
    "            all_p_mean.extend(unit_tbl['p_mean'].tolist())  \n",
    "            all_lat_max_p.extend(unit_tbl['lat_max_p'].tolist()) \n",
    "            all_eu.extend(unit_tbl['euc_max_p'].tolist())\n",
    "            all_corr.extend(unit_tbl['corr_max_p'].tolist()) \n",
    "            peaks = unit_tbl['peak'].values\n",
    "            amp = unit_tbl['amp'].values\n",
    "     \n",
    "        isi_v.extend(unit_tbl['isi_violations_ratio'].tolist())  # ISI violations \n",
    "        snr.extend(unit_tbl['snr'].tolist())  # signal-to-noise ratio\n",
    "        y_loc.extend(unit_tbl['y_loc'].tolist())  # y location of the unit\n",
    "        all_fr.extend(unit_tbl['firing_rate'].tolist())  # firing rate\n",
    "        all_decoder.extend(unit_tbl['decoder_label'].tolist())  # decoder value\n",
    "        if 'tagged_loc' in unit_tbl.columns:\n",
    "            all_tag_loc.extend(unit_tbl['tagged_loc'].tolist())\n",
    "        else:\n",
    "            all_tag_loc.extend([np.nan]*len(unit_tbl))\n",
    "        if 'peak_wf_opt' in unit_tbl.columns:\n",
    "            wf_opt = [wf_opt_unit if isinstance(wf_opt_unit, np.ndarray) else wf_unit for wf_opt_unit, wf_unit in zip(unit_tbl['peak_wf_opt'], unit_tbl['peak_wf'])]  # peak waveform\n",
    "            wf_opt_aligned = [wf_opt_unit if isinstance(wf_opt_unit, np.ndarray) else wf_unit for wf_opt_unit, wf_unit in zip(unit_tbl['peak_wf_opt_aligned'], unit_tbl['peak_wf_aligned'])]  # peak waveform aligned\n",
    "            wf_opt_2d = [wf_opt_unit if isinstance(wf_opt_unit, np.ndarray) else wf_unit for wf_opt_unit, wf_unit in zip(unit_tbl['mat_wf_opt'], unit_tbl['wf_2d'])]  # peak waveform 2D\n",
    "        else:\n",
    "            wf_opt = unit_tbl['peak_wf'].values.tolist()\n",
    "            wf_opt_aligned = unit_tbl['peak_wf_aligned'].values.tolist()\n",
    "            wf_opt_2d = unit_tbl['wf_2d'].values.tolist()\n",
    "\n",
    "        amp_opt = [\n",
    "                        np.max(wf_opt_curr) - np.min(wf_opt_curr) if isinstance(wf_opt_curr, np.ndarray) else curr_amp_unit\n",
    "                        for wf_opt_curr, curr_amp_unit in zip(wf_opt, amp)\n",
    "                    ]   # amplitude of optimized waveforms\n",
    "        if 'amplitude_opt' in unit_tbl.columns:\n",
    "            peak_opt = [\n",
    "                            curr_peak_opt_unit if not np.isnan(curr_peak_opt_unit) else curr_peak_unit\n",
    "                            for curr_peak_opt_unit, curr_peak_unit in zip(list(unit_tbl['amplitude_opt'].values), list(peaks))\n",
    "                        ]\n",
    "        else:\n",
    "            peak_opt = list(peaks)\n",
    "        \n",
    "        if 'peak_waveform_raw_aligned' in unit_tbl.columns:\n",
    "            wf_raw = unit_tbl['peak_waveform_raw_fake_aligned'].values.tolist()\n",
    "            wf_2d_raw = unit_tbl['mat_wf_raw_fake'].values.tolist()\n",
    "            peak_raw = unit_tbl['peak_raw_fake'].values.tolist()\n",
    "            peak_raw = [curr_peak_raw-curr_wf[0] if curr_peak_raw is not None else None for curr_peak_raw, curr_wf in zip(peak_raw, wf_raw)]\n",
    "            amp_raw = unit_tbl['amplitude_raw_fake'].values.tolist()\n",
    "        else:\n",
    "            wf_raw = [None]*len(unit_tbl)\n",
    "            wf_2d_raw = [None]*len(unit_tbl)\n",
    "            peak_raw = [None]*len(unit_tbl)\n",
    "            amp_raw = [None]*len(unit_tbl)\n",
    "        \n",
    "        all_amp.extend(amp_opt)  # amplitude \n",
    "        all_peak.extend(peak_opt)  # peak opto response\n",
    "        all_wf.extend(wf_opt)  # waveform \n",
    "        all_wf_aligned.extend(wf_opt_aligned)  # waveform of max amp\n",
    "        all_wf_2d.extend(wf_opt_2d)  # waveform of max amp\n",
    "        all_wf_raw.extend(wf_raw)  # raw waveform\n",
    "        all_wf_2d_raw.extend(wf_2d_raw)  # raw waveform 2d\n",
    "        all_peak_raw.extend(peak_raw)  # raw peak\n",
    "        all_amp_raw.extend(amp_raw)  # raw amplitude\n",
    "        \n",
    "        top.extend(unit_tbl['LC_range_top'].tolist())\n",
    "        bottom.extend(unit_tbl['LC_range_bottom'].tolist())\n",
    "\n",
    "        # session and opto information\n",
    "        session_df = get_session_tbl(session)\n",
    "        with open(os.path.join(session_dir[f'opto_dir_{data_type}'], f'{session}_opto_info_{target}.json')) as f:\n",
    "            opto_info = json.load(f)\n",
    "        opto_df = pd.read_csv(os.path.join(session_dir[f'opto_dir_{data_type}'], f'{session}_opto_session_{target}.csv'))\n",
    "        if len(opto_df[opto_df['pre_post'] == 'pre'])>0:\n",
    "            pre_end = np.max(opto_df[opto_df['pre_post'] == 'pre']['time'].values)\n",
    "        else:\n",
    "            pre_end = np.nan\n",
    "        \n",
    "        if len(opto_df[opto_df['pre_post'] == 'post'])>0:\n",
    "            post_start = np.min(opto_df[opto_df['pre_post'] == 'post']['time'].values)\n",
    "            post_end = np.max(opto_df[opto_df['pre_post'] == 'post']['time'].values)\n",
    "        else:\n",
    "            post_start = np.nan\n",
    "            post_end = np.nan\n",
    "        \n",
    "        for unit_id in unit_tbl['unit_id'].values:\n",
    "            # append opto_metrics\n",
    "            unit_opto = opto_metrics_session.load_unit(unit_id)\n",
    "            curr_p_resp_all = unit_opto['resp_p_bl'].values\n",
    "            curr_lat_resp_all = unit_opto['resp_lat'].values\n",
    "            curr_p_mean_all = unit_opto['mean_p'].values\n",
    "            curr_eu_all = unit_opto['euclidean_norm'].values\n",
    "            curr_corr_all = unit_opto['correlation'].values\n",
    "\n",
    "            resp_p_all_conditions.append(curr_p_resp_all)\n",
    "            resp_lat_all_conditions.append(curr_lat_resp_all)\n",
    "            corr_all_conditions.append(curr_corr_all)\n",
    "            eu_all_conditions.append(curr_eu_all)\n",
    "            mean_p_all_conditions.append(curr_p_mean_all)\n",
    "\n",
    "            curr_pre_opto = True\n",
    "            curr_post_opto = True \n",
    "            curr_anti_opto = True  \n",
    "            if session_df is not None:\n",
    "                curr_in_beh = True\n",
    "            else:\n",
    "                curr_in_beh = False\n",
    "                curr_trial_count = 0\n",
    "            unit = unit_tbl[unit_tbl['unit_id'] == unit_id]\n",
    "            unit_drift = load_drift(session, unit_id, data_type=data_type)\n",
    "            \n",
    "            if session_df is not None:\n",
    "                go_cue_times = session_df['goCue_start_time']\n",
    "                if unit_drift is not None:\n",
    "                    if unit_drift['ephys_cut'][0] is not None:\n",
    "                        if unit_drift['ephys_cut'][0] > pre_end - 2*60:\n",
    "                            curr_pre_opto = False \n",
    "                        go_cue_times = go_cue_times[go_cue_times >= unit_drift['ephys_cut'][0]]         \n",
    "                    if unit_drift['ephys_cut'][1] is not None:\n",
    "                        if unit_drift['ephys_cut'][1] < post_start + 2*60:\n",
    "                            curr_post_opto = False\n",
    "                        if unit_drift['ephys_cut'][1] < post_end + 2*60:\n",
    "                            curr_anti_opto = False\n",
    "                        go_cue_times = go_cue_times[go_cue_times <= unit_drift['ephys_cut'][1]]\n",
    "                if len(go_cue_times) < 100: \n",
    "                    curr_in_beh = False\n",
    "                curr_trial_count = len(go_cue_times)\n",
    "            opto_tagging_pre.append(curr_pre_opto)\n",
    "            opto_tagging_post.append(curr_post_opto)\n",
    "            anti.append(curr_anti_opto)\n",
    "            trial_count.append(curr_trial_count)\n",
    "            in_behavior.append(curr_in_beh)\n",
    "        probes.extend([df[df['session_id']==session]['probe'].values[0]] * len(unit_tbl))\n",
    "        rec_side.extend([df[df['session_id']==session]['side'].values[0]]*len(unit_tbl))  # recording side\n",
    "        in_df.extend([beh] * len(unit_tbl))  # store session df for each unit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06756496, 0.06756496, 0.06756496, 0.06756496, 0.06756496,\n",
       "       0.06756496])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_p_all_conditions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unit_id', 'bl_max_p', 'p_max', 'p_mean', 'lat_max_p', 'lat_mean',\n",
       "       'euc_max_p', 'corr_max_p', 'opto_pass', 'amp', 'peak', 'real_unit',\n",
       "       'y_loc', 'pass_count', 'spike_times', 'ks_unit_id',\n",
       "       'isi_violations_ratio', 'firing_rate', 'presence_ratio',\n",
       "       'amplitude_cutoff', 'decoder_label', 'depth', 'snr', 'waveform_mean',\n",
       "       'waveform_sd', 'default_qc', 'peak_wf', 'peak_wf_aligned', 'wf_2d',\n",
       "       'LC_range_top', 'LC_range_bottom', 'tagged_loc', 'tagged'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unit_tbl = get_unit_tbl(session, data_type)\n",
    "unit_tbl.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_tagged_units = pd.DataFrame({'session': all_sessions,\n",
    "                                        'unit': all_units,\n",
    "                                        'qc_pass': all_qc_pass_units,\n",
    "                                        'opto_tagged': all_tagged_units,\n",
    "                                        'opto_tagging_pre': opto_tagging_pre,\n",
    "                                        'opto_tagging_post': opto_tagging_post,\n",
    "                                        'anti': anti,\n",
    "                                        'in_df': in_behavior,\n",
    "                                        'trial_count': trial_count,\n",
    "                                        'p_max': all_p_max,\n",
    "                                        'p_mean': all_p_mean,\n",
    "                                        'lat_max_p': all_lat_max_p,\n",
    "                                        'isi_violations': isi_v,\n",
    "                                        'snr': snr,\n",
    "                                        'eu': all_eu,\n",
    "                                        'corr': all_corr,\n",
    "                                        'amp': all_amp,\n",
    "                                        'amp_raw': all_amp_raw,\n",
    "                                        'peak': all_peak,\n",
    "                                        'peak_raw': all_peak_raw,\n",
    "                                        'wf': all_wf,\n",
    "                                        'wf_raw': all_wf_raw,\n",
    "                                        'wf_aligned': all_wf_aligned,\n",
    "                                        'wf_2d': all_wf_2d,\n",
    "                                        'wf_2d_raw': all_wf_2d_raw,\n",
    "                                        'probe': probes,\n",
    "                                        'y_loc': y_loc, \n",
    "                                        'rec_side': rec_side,\n",
    "                                        'top': top,\n",
    "                                        'bottom': bottom,\n",
    "                                        'tag_loc': all_tag_loc,\n",
    "                                        'fr': all_fr,\n",
    "                                        'decoder': all_decoder,\n",
    "                                        'peak_channel_ind': all_peak_channel_ind,\n",
    "                                        'all_p_max': resp_p_all_conditions,\n",
    "                                        'all_p_mean': mean_p_all_conditions,\n",
    "                                        'all_lat_max_p': resp_lat_all_conditions,\n",
    "                                        'all_corr': corr_all_conditions,\n",
    "                                        'all_eu': eu_all_conditions,\n",
    "                                        }\n",
    ")\n",
    "\n",
    "# save dataframe in combined folder\n",
    "with open(os.path.join('/root/capsule/scratch/combined/combine_unit_tbl', 'combined_unit_tbl.pkl'), 'wb') as f:\n",
    "    pickle.dump(combined_tagged_units, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10303"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_tagged_units)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
