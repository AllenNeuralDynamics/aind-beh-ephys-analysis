{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/root/capsule/code/beh_ephys_analysis')\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as patches\n",
    "import json\n",
    "from harp.clock import decode_harp_clock, align_timestamps_to_anchor_points\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "from utils.beh_functions import parseSessionID, session_dirs\n",
    "from utils.plot_utils import shiftedColorMap, template_reorder\n",
    "from open_ephys.analysis import Session##\n",
    "import spikeinterface as si\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.postprocessing as spost\n",
    "import spikeinterface.widgets as sw\n",
    "from aind_dynamic_foraging_basic_analysis.licks.lick_analysis import load_nwb  \n",
    "from aind_ephys_utils import align\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib import colormaps\n",
    "from aind_dynamic_foraging_data_utils.nwb_utils import load_nwb_from_filename\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ephys_opto_preprocessing(session, data_type, target):\n",
    "\n",
    "    # Create a white-to-bright red colormap\n",
    "    colors = [(1, 1, 1), (1, 0, 0)]  # white to red\n",
    "    my_red = LinearSegmentedColormap.from_list(\"white_to_red\", colors)\n",
    "\n",
    "\n",
    "    # %%\n",
    "    print(session)\n",
    "    session_dir = session_dirs(session)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    output_file = os.path.join(session_dir['processed_dir'], f\"{session}_process_record.txt\")\n",
    "    if not os.path.exists(output_file):\n",
    "        log_file = open(output_file, \"w\") \n",
    "    else: \n",
    "        log_file = open(output_file, \"a\")\n",
    "    sys.stdout = log_file\n",
    "    print(f\"Processing {session} data... at {timestamp}\")\n",
    "    with open(os.path.join(session_dir['processed_dir'], f\"{session}_qm.json\"), 'r') as f:\n",
    "        qm = json.load(f)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # ## Ephys recording\n",
    "\n",
    "    # %%\n",
    "    # load neuralpixel session\n",
    "    session_rec = Session(session_dir['session_dir'])\n",
    "    recording = session_rec.recordnodes[0].recordings[0]\n",
    "    timestamps = recording.continuous[0].timestamps\n",
    "    fig = plt.Figure(figsize=(10,2))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(timestamps[:])\n",
    "    plt.title('Timestamps')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(recording.continuous[0].sample_numbers)\n",
    "    plt.title('Sample numbers')\n",
    "    plt.suptitle(f'{session} s: length {len(timestamps)/30000} s')\n",
    "    plt.savefig(os.path.join(session_dir['alignment_dir'], f'{session}_timestamps.pdf'))\n",
    "    plt.show()\n",
    "    qm['ephys_local_sync'] = True\n",
    "\n",
    "    # %%\n",
    "    if np.sum(np.diff(timestamps)<0)/len(timestamps)>0.01:\n",
    "        qm['ephys_local_sync'] = False\n",
    "        # if needs re-alignment\n",
    "        ignore_after_time = min(recording.continuous[0].sample_numbers[0], recording.continuous[0].sample_numbers[-1])/30000 # seconds\n",
    "        if len(recording.continuous) == 3:\n",
    "            recording.add_sync_line(1,            # TTL line number\n",
    "                                    100,          # processor ID\n",
    "                                    'ProbeA-AP',  # stream name\n",
    "                                    main=True,    # set as the main stream\n",
    "                                    ignore_intervals = [(ignore_after_time * 30000, np.inf)])    \n",
    "\n",
    "            recording.add_sync_line(1,            # TTL line number                                       \n",
    "                                    100,          # processor ID\n",
    "                                    'ProbeA-LFP', # stream name\n",
    "                                    ignore_intervals = [(ignore_after_time * 2500, np.inf)])\n",
    "\n",
    "            recording.add_sync_line(1,            # TTL line number\n",
    "                                    103,          # processor ID\n",
    "                                    'PXIe-6341',   # stream name\n",
    "                                    ignore_intervals = [(ignore_after_time * 30000, np.inf)])\n",
    "        else:\n",
    "            recording.add_sync_line(1,            # TTL line number\n",
    "                                    100,          # processor ID\n",
    "                                    'ProbeA',  # stream name\n",
    "                                    main=True,    # set as the main stream\n",
    "                                    ignore_intervals = [(ignore_after_time * 30000, np.inf)])    \n",
    "\n",
    "            recording.add_sync_line(1,            # TTL line number\n",
    "                                    124,          # processor ID\n",
    "                                    'PXIe-6341',   # stream name\n",
    "                                    ignore_intervals = [(ignore_after_time * 30000, np.inf)])\n",
    "\n",
    "        recording.compute_global_timestamps(overwrite=True)\n",
    "        timestamps = recording.continuous[0].timestamps\n",
    "        fig = plt.Figure(figsize=(10,2))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(timestamps[:])\n",
    "        plt.title('Timestamps')\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(recording.continuous[0].sample_numbers)\n",
    "        plt.title('Sample numbers')\n",
    "        plt.suptitle(f'Corrected {session} s: length {len(timestamps)/30000} s')\n",
    "        plt.savefig(os.path.join(session_dir['alignment_dir'], f'{session}_timestamps_corrected.pdf'))\n",
    "        plt.show()\n",
    "\n",
    "    qm['ephys_cut'] = [np.min(timestamps), timestamps[-1]]\n",
    "    # %%\n",
    "    # extract laser times\n",
    "    laser_line = 2\n",
    "    # load all laser times\n",
    "    events = recording.events\n",
    "    del recording\n",
    "    laser_events = events[\n",
    "                    (events.stream_name == 'PXIe-6341')\n",
    "                    & (events.line == laser_line)\n",
    "                    & (events.state == 1)\n",
    "                ].sort_values(by='sample_number')\n",
    "    laser_times = np.sort(laser_events['timestamp'].values)\n",
    "    qm['laser_sync'] = True\n",
    "    # in rare case where only recording is synced by events is not\n",
    "    # if laser_times[-1] < timestamps[0] or laser_times[0] > timestamps[-1]:\n",
    "    #     qm['laser_sync'] = False\n",
    "    #     print(f'{session} laser is not synced.')\n",
    "    #     np_event = events[\n",
    "    #         (events.stream_name == 'ProbeA')\n",
    "    #         & (events.state == 1)\n",
    "    #         & (events.line == 1)\n",
    "    #     ].sort_values(by='sample_number')\n",
    "    #     np_event_time = np.sort(np_event['timestamp'].values)\n",
    "    #     np_global_time = timestamps[np_event['sample_number'].values-recording.continuous[0].sample_numbers[0]]\n",
    "    #     laser_times = align_timestamps_to_anchor_points(laser_times, np_event_time, np_global_time)\n",
    "        # laser_times_ori = laser_times.copy()\n",
    "        # if session == 'behavior_717121_2024-06-15_10-00-58':\n",
    "        #     local_times = np.load('/root/capsule/scratch/717121/behavior_717121_2024-06-15_10-00-58/alignment/events/Neuropix-PXI-100.ProbeA/TTL/original_timestamps.npy')\n",
    "        #     harp_times = np.load('/root/capsule/scratch/717121/behavior_717121_2024-06-15_10-00-58/alignment/events/Neuropix-PXI-100.ProbeA/TTL/timestamps.npy')\n",
    "        # laser_times = align_timestamps_to_anchor_points(laser_times, local_times, harp_times)\n",
    "\n",
    "    # load all laser conditions\n",
    "    opto_dfs = [pd.read_csv(csv) for csv in session_dir['opto_csvs']]\n",
    "    opto_df = pd.concat(opto_dfs)\n",
    "    # load all laser times\n",
    "    plt.plot(laser_times)\n",
    "    plt.axhline(y = np.max(timestamps), color = 'r', linestyle = '--')\n",
    "    plt.axhline(y = np.min(timestamps), color = 'r', linestyle = '--')\n",
    "    plt.title(f'Total trigger: {len(opto_df)}; Total recorded: {len(laser_events)}')\n",
    "    plt.savefig(os.path.join(session_dir['opto_dir'], f'{session}_laser_times.pdf'))\n",
    "\n",
    "    # %%\n",
    "    # adjustment\n",
    "    if len(laser_times) > len(opto_df):\n",
    "        qm['laser_same_count'] = False\n",
    "        print(f'{session} has more laser triggers than opto_df')\n",
    "        laser_times = laser_times[1:]\n",
    "        opto_df = opto_df[:len(laser_times)].copy()\n",
    "        fig = plt.Figure(figsize=(6,3))\n",
    "        plt.plot(laser_times)\n",
    "        plt.axhline(y = np.max(timestamps), color = 'r', linestyle = '--')\n",
    "        plt.axhline(y = np.min(timestamps), color = 'r', linestyle = '--')\n",
    "        plt.title(f'Total trigger: {len(opto_df)}; Total recorded: {len(laser_times)}')\n",
    "    elif len(laser_times) < len(opto_df):\n",
    "        qm['laser_same_count'] = False\n",
    "        print(f'{session} has more opto_df than laser triggers')\n",
    "        opto_df = opto_df[:len(laser_times)].copy()\n",
    "    else:\n",
    "        qm['laser_same_count'] = True\n",
    "        print(f'{session} has equal number of laser triggers and opto_df')\n",
    "    # %% \n",
    "\n",
    "    # %%\n",
    "    sorting = si.load(session_dir[f'curated_dir_{data_type}'])\n",
    "    unit_ids = sorting.get_unit_ids()\n",
    "    unit_spikes  = [timestamps[sorting.get_unit_spike_train(unit_id=unit_id)] for unit_id in unit_ids]\n",
    "    nwb = load_nwb_from_filename(session_dir[f'nwb_dir_{data_type}'])\n",
    "    unit_qc = nwb.units[:][['ks_unit_id', 'isi_violations_ratio', 'firing_rate', 'presence_ratio', 'amplitude_cutoff', 'decoder_label']]\n",
    "\n",
    "    # %%\n",
    "    # load spike times depending on if epyhs is synced\n",
    "    preprosess_qm = os.path.join(session_dir['processed_dir'], f'{session}_qm.json')\n",
    "    if not os.path.exists(preprosess_qm):\n",
    "        print('No preprocessed quality metrics found. Run behavior_and_time_alignment.py frist.')\n",
    "    with open(preprosess_qm, 'r') as f:\n",
    "        preprosess_qm = json.load(f)\n",
    "    # unit_spikes = nwb.units[:]['spike_times']\n",
    "    # unit_ids = nwb.units[:]['ks_unit_id']\n",
    "    if preprosess_qm['ephys_sync']:\n",
    "        print('Ephys synced, getting spike times from nwb')\n",
    "    else:\n",
    "        print('Ephys not synced, resync spike times and laser times')\n",
    "        harp_sync_time = np.load(os.path.join(session_dir['alignment_dir'], 'harp_times.npy'))\n",
    "        local_sync_time = np.load(os.path.join(session_dir['alignment_dir'], 'local_times.npy'))\n",
    "        # to be updated\n",
    "        unit_spikes = [align_timestamps_to_anchor_points(spike_times, local_sync_time, harp_sync_time) for spike_times in unit_spikes]\n",
    "        laser_times = align_timestamps_to_anchor_points(laser_times, local_sync_time, harp_sync_time)\n",
    "    if len(np.where(np.diff(laser_times) > 20*60)[0]) > 0:\n",
    "        # find max time difference\n",
    "        max_ind = np.argmax(np.diff(laser_times))\n",
    "        laser_cut = laser_times[max_ind]\n",
    "        opto_df.loc[laser_times <= laser_cut, 'pre_post'] = 'pre'\n",
    "        opto_df.loc[laser_times > laser_cut, 'pre_post'] = 'post'\n",
    "    else:\n",
    "        opto_df['pre_post'] = 'post'\n",
    "    unit_spikes = {unit_id:unit_spike for unit_id, unit_spike in zip(unit_ids, unit_spikes)}\n",
    "    with open(os.path.join(session_dir[f'ephys_processed_dir_{data_type}'], 'spiketimes.pkl'), 'wb') as f:\n",
    "        pickle.dump(unit_spikes, f)\n",
    "\n",
    "    # %%\n",
    "    # gather all laser information and save\n",
    "    sites = np.sort(opto_df['site'].unique())\n",
    "    sites = [int(i) for i in sites]\n",
    "    if len(sites) == 1:\n",
    "        sites = opto_df['emission_location'].unique()\n",
    "    powers = np.sort(opto_df['power'].unique())\n",
    "    powers = [float(i) for i in powers]\n",
    "    num_pulses = np.sort(opto_df['num_pulses'].unique())\n",
    "    num_pulses = [int(i) for i in num_pulses]\n",
    "    freqs = opto_df['param_group'].unique()\n",
    "    freqs = [int(re.search(r'train(.*?)Hz', freq).group(1)) for freq in freqs]\n",
    "    opto_df['freq'] = opto_df['param_group'].apply(lambda x: int(re.search(r'train(.*?)Hz', x).group(1)))\n",
    "    durations = np.sort(opto_df['duration'].unique())\n",
    "    durations = [float(i) for i in durations]\n",
    "    stim_time = opto_df['pre_post'].unique()\n",
    "    print('Sites:')\n",
    "    print(sites)\n",
    "    print('Powers:')\n",
    "    print(powers)\n",
    "    print('Freqs:')\n",
    "    print(freqs)\n",
    "    print('Pulse durations:')\n",
    "    print(durations)\n",
    "    print('Num pulses:')\n",
    "    print(num_pulses)\n",
    "    print('Stim times')\n",
    "    print(stim_time)\n",
    "    stim_params = {'sites': list(sites) , \n",
    "                    'powers': list(powers), \n",
    "                    'freqs': list(freqs),\n",
    "                    'durations': list(durations),\n",
    "                    'num_pulses': list(num_pulses),\n",
    "                    'pre_post': list(stim_time)}\n",
    "\n",
    "    # %%\n",
    "    # Collect all target laser times and conditions\n",
    "    # save all confirmed laser times\n",
    "    opto_df['time'] = laser_times\n",
    "    laser_onset_samples = np.searchsorted(timestamps, laser_times)\n",
    "    opto_df['laser_onset_samples'] = laser_onset_samples\n",
    "    if 'emission_location' in opto_df.columns and (len(opto_df['site'].unique()) < 2 or 'site' not in opto_df.columns):\n",
    "        opto_df = opto_df.drop(columns=['site'])\n",
    "        opto_df['site'] = opto_df['emission_location']\n",
    "    opto_df.to_csv(os.path.join(session_dir[f'opto_dir_{data_type}'], f'{session}_opto_session.csv'), index=False)\n",
    "\n",
    "    opto_info = stim_params\n",
    "    opto_info['dimensions'] = ['power', 'site', 'num_pulses', 'duration', 'freq', 'pre_post']\n",
    "\n",
    "    with open(os.path.join(session_dir[f'opto_dir_{data_type}'], f'{session}_opto_info.json'), 'w') as f:\n",
    "        json.dump(opto_info, f)\n",
    "\n",
    "    # save all focus laser conditions\n",
    "    if target == 'soma':\n",
    "        resp_win = 25/1000 # seconds\n",
    "        opto_df_target = opto_df.query('site == \"surface\" or site == \"surface_LC\"')\n",
    "        if len(opto_df_target)==0:\n",
    "            opto_df_target = opto_df\n",
    "    elif target == 'axon':\n",
    "        resp_win = 50/1000 # seconds\n",
    "        if 'emission_location' in opto_df.columns:\n",
    "            opto_df_target = opto_df.query('site != \"surface\" and site != \"surface_LC\"')\n",
    "        else:\n",
    "            print('No axon data available')\n",
    "            \n",
    "\n",
    "    opto_df_target.to_csv(os.path.join(session_dir[f'opto_dir_{data_type}'], f'{session}_opto_session_{target}.csv'), index=False)\n",
    "\n",
    "    opto_info_target = {\n",
    "        'sites': [int(x) if isinstance(x, (int, np.integer)) else x for x in opto_df_target['site'].unique()],\n",
    "        'powers': list(map(float,np.sort(opto_df_target['power'].unique()))),\n",
    "        'freqs': list(map(int, np.sort(opto_df_target['freq'].unique()))),  # Convert to Python int\n",
    "        'durations': list(map(int, np.sort(opto_df_target['duration'].unique()))),\n",
    "        'num_pulses': list(map(int, np.sort(opto_df_target['num_pulses'].unique()))),\n",
    "        'pre_post': list(np.sort(opto_df_target['pre_post'].unique()))  # Convert to Python int\n",
    "    }\n",
    "\n",
    "    opto_info_target['dimensions'] = opto_info['dimensions']\n",
    "    opto_info_target['resp_win'] = resp_win\n",
    "\n",
    "\n",
    "    # write json file\n",
    "    with open(os.path.join(session_dir[f'opto_dir_{data_type}'], f'{session}_opto_info_{target}.json'), 'w') as f:\n",
    "        json.dump(opto_info_target, f)\n",
    "\n",
    "\n",
    "    # %%\n",
    "    dim_len = [len(opto_df_target[dim].unique()) for dim in opto_info['dimensions']]\n",
    "    print(f'Saving opto_responses: {session}')\n",
    "    \n",
    "    resp_p = {}\n",
    "    resp_lat = {}\n",
    "    for curr_id in unit_ids:\n",
    "        spike_times = unit_spikes[curr_id]\n",
    "        curr_resp_p = np.empty(tuple(dim_len), dtype=object)\n",
    "        curr_resp_lat = np.empty(tuple(dim_len), dtype=object)\n",
    "        for power_ind, curr_power in enumerate(np.sort(opto_df_target['power'].unique())):\n",
    "            for site_ind, curr_site in enumerate(opto_df_target['site'].unique()):                                                             \n",
    "                for duration_ind, curr_duration in enumerate(np.sort(opto_df_target['duration'].unique())):\n",
    "                    for freq_ind, curr_freq in enumerate(np.sort(opto_df_target['freq'].unique())):\n",
    "                        for stim_time_ind, curr_stim_time in enumerate(opto_df_target['pre_post'].unique()):\n",
    "                            for num_pulse_ind, curr_num_pulses in enumerate(np.sort(opto_df_target['num_pulses'].unique())):\n",
    "                                laser_times_curr = opto_df_target.query('site == @curr_site and power == @curr_power and duration == @curr_duration and freq == @curr_freq and num_pulses == @curr_num_pulses and pre_post ==@curr_stim_time')['time'].values\n",
    "                                if len(laser_times_curr) == 0:\n",
    "                                    curr_resp_p[power_ind, site_ind, num_pulse_ind, duration_ind, freq_ind, stim_time_ind] = np.full(curr_num_pulses, np.nan).tolist()\n",
    "                                    curr_resp_lat[power_ind, site_ind, num_pulse_ind, duration_ind, freq_ind, stim_time_ind] = np.full(curr_num_pulses, np.nan).tolist()\n",
    "                                else:\n",
    "                                    resp_temp = []\n",
    "                                    resp_lat_temp = []\n",
    "                                    for curr_pulse in range(curr_num_pulses):\n",
    "                                        laser_times_curr_pulse = laser_times_curr + curr_pulse * 1/curr_freq\n",
    "                                        df = align.to_events(spike_times, laser_times_curr_pulse, (0, resp_win), return_df=True)\n",
    "                                        resp_temp.append(len(df['event_index'].unique())/len(laser_times_curr_pulse))\n",
    "                                        if len(df) > 0:\n",
    "                                            resp_lat_temp.append(np.nanmean(df.groupby('event_index')['time'].min().values))\n",
    "                                        else:\n",
    "                                            resp_lat_temp.append(np.nan)\n",
    "                                    \n",
    "                                    curr_resp_lat[power_ind, site_ind, num_pulse_ind, duration_ind, freq_ind, stim_time_ind] = resp_lat_temp\n",
    "                                    curr_resp_p[power_ind, site_ind, num_pulse_ind, duration_ind, freq_ind, stim_time_ind] = resp_temp\n",
    "        resp_p[curr_id] = curr_resp_p\n",
    "        resp_lat[curr_id] = curr_resp_lat\n",
    "        # save to unit_opto_tag\n",
    "        # np.save(os.path.join(session_dir[f'opto_dir_{data_type}'], f'unit_opto_tag_p_{target}_{curr_id}.npy'), curr_resp_p)  \n",
    "        # np.save(os.path.join(session_dir[f'opto_dir_{data_type}'], f'unit_opto_tag_lat_{target}_{curr_id}.npy'), curr_resp_lat)  \n",
    "        # np.save(os.path.join(session_dir[f'opto_dir_{data_type}'], f'spiketimes_{curr_id}.npy'), spike_times)    \n",
    "        # print(f'Unit {curr_id} done')\n",
    "\n",
    "    opto_response = {'resp_p': resp_p, 'resp_lat': resp_lat}\n",
    "\n",
    "    with open(os.path.join(session_dir[f'opto_dir_{data_type}'], f'{session}_opto_responses_{target}.pkl'), 'wb') as f:\n",
    "        pickle.dump(opto_response, f)\n",
    "\n",
    "    print(f'Saved opto_responses: {session}')\n",
    "        \n",
    "\n",
    "\n",
    "    # %%\n",
    "    # load waveforms info\n",
    "    we = si.load(session_dir[f'postprocessed_dir_{data_type}'], load_extensions=False)\n",
    "    print(f'Loaded session: {session}')\n",
    "    unit_ids = we.sorting.get_unit_ids()\n",
    "    all_templates = we.get_extension(\"templates\").get_data(operator=\"average\")\n",
    "    all_channels = we.sparsity.channel_ids\n",
    "    if all_channels[0].startswith('AP'):\n",
    "        all_channels_int = np.array([int(channel.split('AP')[-1]) for channel in all_channels])\n",
    "    else:\n",
    "        all_channels_int = np.array([int(channel.split('CH')[-1]) for channel in all_channels])\n",
    "    unit_spartsiity = we.sparsity.unit_id_to_channel_ids\n",
    "    channel_locations = we.get_channel_locations()\n",
    "    unit_locations = we.get_extension(\"unit_locations\").get_data(outputs=\"by_unit\")\n",
    "    del we\n",
    "    right_left = channel_locations[:, 0]<20\n",
    "\n",
    "    # re-organize templates so that left and right separate\n",
    "    colors = [\"blue\", \"white\", \"red\"]\n",
    "    b_w_r_cmap = LinearSegmentedColormap.from_list(\"b_w_r\", colors)\n",
    "\n",
    "    y_neighbors_to_keep = 3\n",
    "    samples_to_keep = [-30, 60]\n",
    "    orginal_loc = False\n",
    "    waveform_params = {'samples_to_keep': samples_to_keep, 'y_neighbors_to_keep': y_neighbors_to_keep, 'orginal_loc': orginal_loc}\n",
    "    with open(os.path.join(session_dir[f'opto_dir_{data_type}'], f'{session}_waveform_params.json'), 'w') as f:\n",
    "        json.dump(waveform_params, f)\n",
    "\n",
    "    channel_loc_dict = {channel: channel_loc for channel, channel_loc in zip(all_channels_int, channel_locations)}\n",
    "\n",
    "    # save all re-ordered templates\n",
    "    print(f'Saving templates: {session}')\n",
    "    opto_waveforms = {}\n",
    "    for unit_ind, unit_id in enumerate(unit_ids):\n",
    "        curr_template = all_templates[unit_ind]\n",
    "        reordered_template = template_reorder(curr_template, right_left, all_channels_int, sample_to_keep = samples_to_keep, y_neighbors_to_keep = y_neighbors_to_keep, orginal_loc = orginal_loc)\n",
    "        # shifted_cmap = shiftedColorMap(custom_cmap, np.nanmin(reordered_template), np.nanmax(reordered_template), 'shifted_b_w_r')\n",
    "        # plt.imshow(reordered_template, extent = [-30, -30+2*(30+60), 2*3+1, 0], cmap=shifted_cmap, aspect='auto');\n",
    "        # plt.axvline(0, color='black', linestyle='--', linewidth=0.5)\n",
    "        # plt.axvline(30+60, color='black', linestyle='--', linewidth=0.5)\n",
    "        # plt.title(f'Unit_id: {unit_id} depth: {unit_locations[unit_id][1]:.2f}')\n",
    "        # plt.box(False)\n",
    "        # plt.colorbar();\n",
    "        # np.save(os.path.join(session_dir[f'opto_dir_{data_type}'], f'unit_waveform_{unit_id}.npy'), reordered_template)\n",
    "        opto_waveforms[unit_id] = reordered_template\n",
    "\n",
    "    with open(os.path.join(session_dir[f'opto_dir_{data_type}'], f'{session}_waveforms_{target}.pkl'), 'wb') as f:\n",
    "        pickle.dump(opto_waveforms, f)\n",
    "    print(f'Saved templates: {session}')\n",
    "\n",
    "    # %%\n",
    "    # %%\n",
    "    qm_file = os.path.join(session_dir['processed_dir'], f\"{session}_qm.json\")\n",
    "    with open(qm_file, 'w') as f:\n",
    "        json.dump(qm, f, indent=4)\n",
    "    print(f\"Output saved to {output_file}\")\n",
    "    sys.stdout = sys.__stdout__\n",
    "    # Close the file\n",
    "    log_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
